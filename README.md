You can follow this kaggle notebook (https://www.kaggle.com/code/happy1scientist/notebooke7aca9a4f8/notebook) on how to run the code.

If you want to test a different set of ARC tasks, check out how build_challenges_v2 and v2_eval_challenges load the ARC-AGI-2 public eval tasks in src/data.py. After you make the changes to load the new set, replace all references to v2_eval_challenges with the new set in src/main.py.

=====================================================================================================

Building upon the solution of ARC-AGI 1 SoTA by Jeremy Berman (https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi), I was also inspired by DreamCoder (https://arxiv.org/abs/2006.08381) in which a library of advanced functions is iteratively built up and then used to find solution to a task faster. While DreamCoder requires hand-crafted Domain Design Language (DSL), I use LLM to bootstrap the initial library and improving existing functions in the library. Starting with an empty library, I call a LLM to ask for a python program that can potentially solve an ARC task, and add the best program (defined by having the best accuracy on training examples) to the library. For future tasks, I first fetch the best program from the library and include its code, its output with respect to the training input, and its output difference with the training output in the LLM prompt and ask the LLM to evolve the program.

I first perform the process described above on ARC-AGI 2 train set to build up a library of 538 "primitives" (note: while there are 1000 training examples, sometimes the LLM call timeouts. In such cases no programs get added to the library). And then I use the result library to attempt ARC-AGI 2 eval tasks following the same process, with five equivalent LLM calls generated per task and after looping through all the eval task, I start with the first one again till the end (two rounds total).

While both my solution and Berman’s relies on prompting LLM to evolve python program, Berman’s solution treats each ARC task as isolated problem. If a correct program is found, it is never used to jump start the search for the next problem. My solution leverages the knowledge learned from previous ARC tasks to tackle the next one and has a more efficient search (Berman’s solution generated up to 500 functions per task while mine generated 10).